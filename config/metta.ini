[base]
package = metta
env_name = metta 
policy_name = Policy
rnn_name = Recurrent
vec = multiprocessing

[env]
render_mode = auto
#num_envs = 128 

[train]
total_timesteps = 75_000_000
#num_envs = 256
num_envs = 128
num_workers = 16
#env_batch_size = 128
env_batch_size = 64
#learning_rate = 0.0005
learning_rate = 0.001
gamma = 0.97
gae_lambda = 0.90
ent_coef = 0.002
anneal_lr = False
batch_size = 262144
minibatch_size = 16384

#total_timesteps = 107000000000
#checkpoint_interval = 1000
#learning_rate = 0.0004573146765703167
#num_envs = 2
#num_workers = 2
#env_batch_size = 1
#update_epochs = 1
#gamma = 0.7647543366891623
#gae_lambda = 0.996005622445478
#ent_coef = 0.01210084358004069
#max_grad_norm = 0.6075578331947327
#vf_coef = 0.3979089612467003
#bptt_horizon = 32
#batch_size = 262144
#minibatch_size = 32768
#compile = False

[sweep]
method = protein
name = sweep

[sweep.metric]
goal = maximize
name = score 
min = 0
max = 10
scale = auto

#[sweep.train.total_timesteps]
#distribution = log_normal
#min = 2e7
#max = 1e8
#mean = 5e7
#scale = auto
