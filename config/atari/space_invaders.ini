[base]
package = atari
env_name = beam_rider
policy_name = Policy
rnn_name = Recurrent

[train]
num_envs = 144
num_workers = 24
env_batch_size = 48
zero_copy = False
batch_size = 65_536
minibatch_size = 4,096
update_epochs = 4
bptt_horizon = 8
total_timesteps = 7_500_000
learning_rate = 0.0012161186756094724
gae_lambda = 0.6957035398791592
gamma = 0.9925043678586688
ent_coef = 0.032082891906869346
clip_coef = 0.24504077831570073
vf_clip_coef = 0.18204547640437296
vf_coef = 0.5850012910005633
max_grad_norm = 0.6649078130722046
anneal_lr = False

[env]
frameskip = 4
repeat_action_probability = 0.0

[sweep.parameters.env.parameters.frameskip]
distribution = uniform
min = 1
max = 10

#[sweep.parameters.env.parameters.repeat_action_probability]
#distribution = uniform
#min = 0
#max = 1
 
[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 5_000_000
max = 25_000_000

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 16384
max = 65536

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 512
max = 8192
