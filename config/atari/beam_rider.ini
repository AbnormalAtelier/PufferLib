[base]
package = atari
env_name = beam_rider
policy_name = Policy
rnn_name = Recurrent

[train]
num_envs = 144
num_workers = 24
env_batch_size = 48
zero_copy = False
batch_size = 65_536
minibatch_size = 1024
update_epochs = 3
bptt_horizon = 2
total_timesteps = 10_000_000
learning_rate = 0.00041171401568673385
gae_lambda = 0.14527976163861273
gamma = 0.990622479610104
ent_coef = 0.010996558409985507
clip_coef = 0.4966414480536032
vf_clip_coef = 0.13282356641582535
vf_coef = 0.985913502481555
max_grad_norm = 0.9385297894477844
anneal_lr = False

[env]
frameskip = 4
repeat_action_probability = 0.0

[sweep.parameters.env.parameters.frameskip]
distribution = uniform
min = 1
max = 10

#[sweep.parameters.env.parameters.repeat_action_probability]
#distribution = uniform
#min = 0
#max = 1
 
[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 5_000_000
max = 25_000_000

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 16384
max = 65536

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 512
max = 8192
