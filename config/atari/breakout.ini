[base]
package = atari
env_name = breakout
policy_name = Policy
rnn_name = Recurrent

[train]
num_envs = 144
num_workers = 24
env_batch_size = 48
zero_copy = False
batch_size = 32768
minibatch_size = 2048
update_epochs = 3
bptt_horizon = 8
total_timesteps = 5_000_000
learning_rate = 0.0006362237073612932
gae_lambda = 0.9693675324249582
gamma = 0.9225000730890304
ent_coef = 0.007819683794893506
clip_coef = 0.21999888278763577
vf_clip_coef = 0.1870618596142218
vf_coef = 0.8674563653606585
max_grad_norm = 0.7610697746276855
anneal_lr = False

[env]
frameskip = 4
repeat_action_probability = 0.0

[sweep.parameters.env.parameters.frameskip]
distribution = uniform
min = 1
max = 10

#[sweep.parameters.env.parameters.repeat_action_probability]
#distribution = uniform
#min = 0
#max = 1
 
[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 5_000_000
max = 25_000_000

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 16384
max = 65536

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 512
max = 8192
