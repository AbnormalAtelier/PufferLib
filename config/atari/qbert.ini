[base]
package = atari
env_name = beam_rider
policy_name = Policy
rnn_name = Recurrent

[train]
num_envs = 144
num_workers = 24
env_batch_size = 48
zero_copy = False
batch_size = 32_768
minibatch_size = 1024
update_epochs = 3
bptt_horizon = 8
total_timesteps = 15_000_000
learning_rate = 0.00104284086325656
gae_lambda = 0.8573007456819492
gamma = 0.9426362777287904
ent_coef = 0.025180053429464784
clip_coef = 0.23123278532103236
vf_clip_coef = 0.12751979973690886
vf_coef = 0.5903166418793799
max_grad_norm = 0.1610541045665741
anneal_lr = False

[env]
frameskip = 4
repeat_action_probability = 0.0

[sweep.parameters.env.parameters.frameskip]
distribution = uniform
min = 1
max = 10

#[sweep.parameters.env.parameters.repeat_action_probability]
#distribution = uniform
#min = 0
#max = 1
 
[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 5_000_000
max = 25_000_000

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 16384
max = 65536

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 512
max = 8192
