[base]
package = None
env_name = None
policy_name = Policy
rnn_name = None
max_suggestion_cost = 3600

[env]
[policy]
[rnn]

[train]
seed =  1
torch_deterministic = True
cpu_offload = False
device = cuda
total_timesteps = 10_000_000
learning_rate = 2.5e-4
anneal_lr = True
gamma = 0.99
gae_lambda = 0.95
update_epochs = 4
norm_adv = True
clip_coef = 0.1
clip_vloss = True
vf_coef = 0.5
vf_clip_coef = 0.1
max_grad_norm = 0.5
ent_coef = 0.01
target_kl = None

num_envs = 8
num_workers = 8
env_batch_size = None
zero_copy = True
data_dir = experiments
checkpoint_interval = 200
batch_size = 1024
minibatch_size = 512
bptt_horizon = 16
compile = False
compile_mode = reduce-overhead

[sweep]
method = random
name = sweep

[sweep.metric]
goal = maximize
name = environment/episode_return

[sweep.train.learning_rate]
distribution = log_normal
mean = 0.005
scale = 1.0
clip = 2.0

[sweep.train.ent_coef]
distribution = log_normal
mean = 0.005
scale = 0.5
clip = 1.0

[sweep.train.gamma]
distribution = logit_normal
mean = 0.98
scale = 0.5
clip = 1.0

[sweep.train.gae_lambda]
distribution = logit_normal
mean = 0.95
scale = 0.5
clip = 1.0

[sweep.train.update_epochs]
distribution = int_uniform
min = 1
max = 4

[sweep.train.vf_coef]
distribution = uniform
min = 0.0
max = 1.0

[sweep.train.max_grad_norm]
distribution = uniform
min = 0.0
max = 10.0

[sweep.train.bptt_horizon]
distribution = uniform_pow2
min = 1
max = 32
