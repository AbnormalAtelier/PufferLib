[base]
package = ocean
env_name = moba
policy_name = MOBA
rnn_name = Recurrent

[train]
total_timesteps = 250_000_000
checkpoint_interval = 50
learning_rate = 0.0020617122808362733
num_envs = 8
num_workers = 8
env_batch_size = 4
update_epochs = 4
gamma = 0.9334470461385748
gae_lambda = 0.9948012594040528
clip_coef = 0.18948096481167295
vf_clip_coef = 0.0020862799862927427
vf_coef = 0.9250065817752148
ent_coef = 0.00010707377466427852
max_grad_norm = 0.3052051067352295
bptt_horizon = 2
batch_size = 2_097_152
minibatch_size = 32_768
compile = False
anneal_lr = False
device = cuda

[env]
reward_death = -0.7585290670394897
reward_xp = 0.011239750683307648
reward_distance = 0.34798964858055115
reward_tower = 4.577837944030762

[sweep.metric]
goal = maximize
name = environment/elo

[sweep.parameters.env.parameters.reward_death]
distribution = uniform
min = -5.0
max = 0

[sweep.parameters.env.parameters.reward_xp]
distribution = uniform
min = 0.0
max = 0.05

[sweep.parameters.env.parameters.reward_distance]
distribution = uniform
min = 0.0
max = 0.5

[sweep.parameters.env.parameters.reward_tower]
distribution = uniform
min = 0.0
max = 5.0
 
[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 100_000_000
max = 10_000_000_000

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 512000
max = 2048000

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 16000
max = 128000

