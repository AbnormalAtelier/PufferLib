[base]
package = ocean
env_name = my_pong
; rnn_name = Recurrent

[env]
num_envs = 4096

[train]
total_timesteps = 20_000_000_000
num_envs = 2
num_workers = 2
env_batch_size = 1
batch_size = 1024000
minibatch_size = 10240
update_epochs = 2
bptt_horizon = 8
anneal_lr = False
gae_lambda = 0.9590507508564148
gamma = 0.9671759718055382
clip_coef = 0.3031963355045393
clip_vloss = True
vf_coef = 0.9274225135298954
vf_clip_coef = 0.13369578727174328
max_grad_norm = 1.392141580581665
ent_coef = 0.01557519441744131
learning_rate = 0.0006112614226003401
checkpoint_interval = 200
norm_adv = True
device = cpu
compile = False
cpu_offload = False
compile_mode = "reduce-overhead"

[sweep.metric]
goal = maximize
name = environment/reward

[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 5_000_000
max = 25_000_000

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 16384
max = 65536

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 512
max = 8192
