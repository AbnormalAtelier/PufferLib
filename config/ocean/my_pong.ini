[base]
package = ocean
env_name = my_pong
; rnn_name = Recurrent

[env]
num_envs = 4096

[train]
device = cpu
checkpoint_interval = 200
total_timesteps = 40_000_000
num_envs = 2
num_workers = 2
env_batch_size = 1
batch_size = 1_048_576
minibatch_size = 8_192
update_epochs = 3
bptt_horizon = 8
learning_rate = 0.013375535642164845
gamma = 0.992728622503938
gae_lambda = 0.9988108387467688
vf_coef = 0.11742848626702285
vf_clip_coef = 0.31951195971202845
clip_coef = 0.40020719726615295
ent_coef = 0.0016763474659573384
max_grad_norm = 0.34648019075393677
anneal_lr = False
clip_vloss = True
norm_adv = True

[sweep.metric]
goal = maximize
name = environment/winrate
; name = environment/reward

[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 20_000_000
max = 200_000_000

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 16384
max = 1048576

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 1024
max = 65536
