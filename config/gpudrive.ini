[base]
package = gpudrive
env_name = gpudrive
policy_name = Policy
rnn_name = Recurrent

[train]
total_timesteps = 20_000_000
num_envs = 1
num_workers = 1
env_batch_size = 1
zero_copy = False
batch_size = 524288
update_epochs = 2
minibatch_size = 32768
bptt_horizon = 16
anneal_lr = False
gae_lambda = 0.95
gamma = 0.95
clip_coef = 0.2
vf_coef = 0.5
vf_clip_coef = 0.2
max_grad_norm = 0.5
ent_coef = 0.01
learning_rate = 0.0001
checkpoint_interval = 1000
device = cuda

[sweep.metric]
goal = maximize
name = environment/goal_achieved

[sweep.parameters.env.parameters.num_worlds]
distribution = uniform
min = 16 
max = 512

[sweep.parameters.env.parameters.max_episode_steps]
distribution = uniform
min = 100
max = 1000

[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 20_000_000
max = 25_000_000

[sweep.parameters.train.parameters.batch_size]
distribution = uniform
min = 32768
max = 524288

[sweep.parameters.train.parameters.minibatch_size]
distribution = uniform
min = 2048
max = 32768


