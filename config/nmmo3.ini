[base]
package = nmmo3
env_name = nmmo3
rnn_name = Recurrent

[train]
total_timesteps = 894435328
checkpoint_interval = 1000
learning_rate = 0.0009256044179473068
num_envs = 2
num_workers = 2
env_batch_size = 1
update_epochs = 1
gamma = 0.9091745370458072
gae_lambda = 0.678138068784309
ent_coef = 0.03506925372378017
max_grad_norm = 0.33368468284606934
vf_coef = 0.0846873339924445
bptt_horizon = 8
batch_size = 262144
minibatch_size = 65536
compile = False
anneal_lr = False

[env]
reward_combat_level = 1.7163633108139038
reward_prof_level = 0.9741113781929016
reward_item_level = 0.46555063128471375
reward_market = 0.13975507020950317
reward_death_mmo = 0.2346312403678894

[sweep.metric]
goal = maximize
name = environment/min_comb_prof

[sweep.parameters.env.parameters.reward_combat_level]
distribution = uniform
min = 0.0
max = 5.0

[sweep.parameters.env.parameters.reward_prof_level]
distribution = uniform
min = 0.0
max = 5.0

[sweep.parameters.env.parameters.reward_item_level]
distribution = uniform
min = 0.0
max = 5.0

[sweep.parameters.env.parameters.reward_market]
distribution = uniform
min = 0.0
max = 1.0

[sweep.parameters.env.parameters.reward_death_mmo]
distribution = uniform
min = 0.0
max = 5.0

[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 300_000_000
max = 100_000_000_000
