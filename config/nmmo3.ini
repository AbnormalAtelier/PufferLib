[base]
package = nmmo3
env_name = nmmo3
rnn_name = Recurrent

[train]
total_timesteps = 149159936
checkpoint_interval = 1000
learning_rate = 0.00015697942823402757
num_envs = 2
num_workers = 2
env_batch_size = 1
update_epochs = 2
gamma = 0.836237738368539
gae_lambda = 0.905539544654982
ent_coef = 0.017310410099593155
max_grad_norm = 0.4244130849838257
vf_coef = 0.34014830787683303
bptt_horizon = 16
batch_size = 262144
minibatch_size = 65536
compile = False
anneal_lr = False

[env]
reward_combat_level = 1.2009227275848389
reward_prof_level = 2.05468225479126
reward_item_level = 0.4820292890071869
reward_market = 0.15248291194438934
reward_death_mmo = 0.7844899296760559

[sweep.metric]
goal = maximize
name = environment/min_comb_prof

[sweep.parameters.env.parameters.reward_combat_level]
distribution = uniform
min = 0.0
max = 5.0

[sweep.parameters.env.parameters.reward_prof_level]
distribution = uniform
min = 0.0
max = 5.0

[sweep.parameters.env.parameters.reward_item_level]
distribution = uniform
min = 0.0
max = 5.0

[sweep.parameters.env.parameters.reward_market]
distribution = uniform
min = 0.0
max = 1.0

[sweep.parameters.env.parameters.reward_death_mmo]
distribution = uniform
min = 0.0
max = 5.0

[sweep.parameters.train.parameters.total_timesteps]
distribution = uniform
min = 300_000_000
max = 100_000_000_000
