[base]
package = ocean
env_name = puffer_gpudrive
policy_name = GPUDrive
rnn_name = Recurrent

[vec]
num_workers = 8
num_envs = 8
batch_size = 2
#backend = Serial

[policy]
input_size = 64
hidden_size = 512

[rnn]
input_size = 512
hidden_size = 512

[env]
num_agents = 1024
reward_vehicle_collision = -0.6332580242253343
reward_offroad_collision = -0.5667510952286603
spawn_immunity_timer = 49.50051968750317
reward_goal_post_respawn = 0.8461654474116196
reward_vehicle_collision_post_respawn = -0.024841538797994778
resample_frequency = 91
num_maps = 100

[train]
total_timesteps = 76230823
#learning_rate = 0.02
#gamma = 0.985
anneal_lr = True
batch_size = 745472
minibatch_size = 11648
max_minibatch_size = 11648
bptt_horizon = 91
adam_beta1 = 0.9520792628086348
adam_beta2 = 0.9993108421512796
adam_eps = 2.8702170460034483e-9
clip_coef = 0.6085563413539425
ent_coef = 0.00872354862269773
gae_lambda = 0.9662493307017929
gamma = 0.9775776428338787
learning_rate = 0.011485508427569579
max_grad_norm = 1.65023224210151
prio_alpha = 0.9534334655417702
prio_beta0 = 0.9725215839215794
update_epochs = 1
vf_clip_coef = 1.39932891252331
vf_coef = 3.061555813720951
vtrace_c_clip = 0.9344915148832842
vtrace_rho_clip = 1.1673898472161945
checkpoint_interval = 1000



[sweep.train.total_timesteps]
distribution = log_normal
min = 5e7
max = 2e8
mean = 1e8
scale = time
 
[sweep.env.reward_vehicle_collision]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.2
scale = auto 
 
[sweep.env.reward_offroad_collision]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.2
scale = auto

[sweep.env.spawn_immunity_timer]
distribution = uniform
min = 1
max = 91
mean = 30
scale = auto

[sweep.env.reward_goal_post_respawn]
distribution = uniform
min = 0.0
max = 1.0
mean = 0.5
scale = auto

[sweep.env.reward_vehicle_collision_post_respawn]
distribution = uniform
min = -1.0
max = 0.0
mean = -0.2
scale = auto
