[base]
package = metta
env_name = metta 
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 256
num_workers = 16
batch_size = 128

[env]
render_mode = auto

[train]
total_timesteps = 400_000_000
learning_rate = 0.0013848535655657842
gamma = 0.9959746852829785
gae_lambda = 0.9283720217357007
ent_coef = 0.0008901028045115906
max_grad_norm = 1.2453426220454547
vf_coef = 1.0103141121889738
anneal_lr = False
batch_size = 393216
minibatch_size = 16384

adam_beta1 = 0.8597471085735918
adam_beta2 = 0.9998113998134229
adam_eps = 0.000249501214984291

#total_timesteps = 107000000000
#checkpoint_interval = 1000
#learning_rate = 0.0004573146765703167
#num_envs = 2
#num_workers = 2
#env_batch_size = 1
#update_epochs = 1
#gamma = 0.7647543366891623
#gae_lambda = 0.996005622445478
#ent_coef = 0.01210084358004069
#max_grad_norm = 0.6075578331947327
#vf_coef = 0.3979089612467003
#bptt_horizon = 32
#batch_size = 262144
#minibatch_size = 32768
#compile = False

#[sweep.train.total_timesteps]
#distribution = log_normal
#min = 2e7
#max = 1e8
#mean = 5e7
#scale = auto
