[base]
package = metta
env_name = metta 
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 512
num_workers = 16
batch_size = 128

[env]
render_mode = auto

[train]
total_timesteps = 1_000_000_000
learning_rate = 0.0013848535655657842
gamma = 0.9959746852829785
gae_lambda = 0.9283720217357007
ent_coef = 0.0008901028045115906
max_grad_norm = 1.2453426220454547
vf_coef = 1.0103141121889738
anneal_lr = False
batch_size = 524288
minibatch_size = 32768

adam_beta1 = 0.8597471085735918
adam_beta2 = 0.9998113998134229
adam_eps = 0.000249501214984291

[sweep.train.total_timesteps]
distribution = log_normal
min = 1e8
max = 5e8
mean = 1e8
scale = auto

[sweep.env.ore_reward]
distribution = uniform
min = 0.0
mean = 0.25
max = 1.0
scale = auto

[sweep.env.heart_reward]
distribution = uniform
min = 0.0
mean = 0.5
max = 1.0
scale = auto

[sweep.env.battery_reward]
distribution = uniform
min = 0.0
mean = 0.25
max = 1.0
scale = auto


